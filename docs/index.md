### Prerequisites

A Data Product should already exist in order to attach the new components to it.

### Component basic Information

This section includes the basic information that any Component of must have:

- Name: required name used for display purposes on your Data Product
- Fully Qualified Name: workload fully qualified name, this is optional as will be generated by the system if not given by you
- Description: a short description to help others understand what this Workload is for.
- Domain: the Domain of the Data Product this Workload belongs to. Be sure to choose it correctly as is a fundamental part of the Workload and cannot be changed afterwards.
- Data Product: the Data Product this Workload belongs to, be sure to choose the right one.
- Identifier: unique ID for this new entity inside the domain. Don't worry to fill this field, it will be automatically filled for you.
- Development Group: development group of this Data Product. Don't worry to fill this field, it will be automatically filled for you.
- Depends On: if you want your workload to depend on other components from the Data Product, you can choose this option (Optional).
- Reads From: is filled only for DataPipeline workloads, and it represents the list of output ports or external systems that is reading.

*Example:*

| Field name               | Example value                                                                                          |
|:-------------------------|:-------------------------------------------------------------------------------------------------------|
| **Name**                 | Spark CDP Workload                                                                                     |
| **Fully Qualified Name** | Spark CDP Workload                                                                                     | 
| **Description**          | Creates a Spark CDP Workload to ...                                                                    |
| **Domain**               | domain:healthcare                                                                                      |
| **Data Product**         | system:healthcare.spark-cdp-test.0                                                                     |
| ***Identifier***         | Will look something like this: *healthcare.spark-cdp-test.0.spark-cdp-workload*                        |
| ***Development Group***  | Might look something like this: *group:datameshplatform* Depends on the Data Product development group |
| **Depens On**            | Spark CDP Workload v0                                                                                  |
| **Reads From**           | Res1                                                                                                   |


### Spark infrastructure details
- **CDE Service:** The long-running Kubernetes cluster and services that manage the virtual clusters.
- **CDE Virtual Cluster:** An individual auto-scaling cluster with defined CPU and memory ranges. Jobs are associated with clusters.
- **Artifact bucket:** S3 Bucket name where spark artifacts will be stored.
- **Job name:** The name of the job. A job is an application code along with defined configurations and resources.

Learn more about it on the [official Cloudera documentation](https://docs.cloudera.com/data-engineering/cloud/overview/topics/cde-service-overview.html).


*Example:*

| Field name              | Example value         |
|:------------------------|:----------------------|
| **CDE Service**         | my-cde-service        |
| **CDE Virtual Cluster** | cdp-cluster-123       |
| **Artifact bucket**     | spark-artifact-bucket |
| **Job name**            | spark-job-123         |


### Spark job details
- **Driver Cores:** [Optional]  Number of cores to use for the driver process. If specified must be a positive integer.
- **Driver memory:** [Optional] Amount of memory to use for the driver process. If specified must be a positive integer followed by 'g' e.g. 4g.
- **Executor Cores:** [Optional] The number of cores to use on each executor. If specified must be a positive integer
- **Executor memory:** [Optional] Amount of memory to use per executor process. If specified must be a positive integer followed by 'g' e.g. 8g.
- **Number of executors:** [Optional] Number of executors to run. If specified must be a positive integer.

Learn more about it on the [official Spark documentation](https://spark.apache.org/docs/latest/configuration.html).

*Example:*

| Field name              | Example value |
|:------------------------|:--------------|
| **Driver Cores**        | 2             |
| **Driver memory**       | 4g            |
| **Executor Cores**      | 4             |
| **Executor memory**     | 8g            |
| **Number of executors** | 10            |


### Spark job scheduling
Spark jobs can optionally be scheduled so that they are automatically run on an interval.
- 
- **Cron expression:** [Optional] A cron expression that is directly provided to the scheduler. This field allows for detailed time-based scheduling, specifying when and how often the job should be executed. If not provided, the job will not be automatically executed based on a time-based schedule.
- **Start Date:** [Optional] The date and time when a scheduled Spark job should begin its execution. If not provided, the job can start immediately upon scheduling.
- **End Date:** [Optional] The date and time when a scheduled Spark job should stop its execution. If not provided, the job may continue running indefinitely according to the schedule.

*Example:*

| Field name          | Example value    |
|:--------------------|:-----------------|
| **Cron expression** | 0 */1 * * *      |
| **Start Date**      | 2023-08-17 08:00 |
| **End Date**        | 2023-09-01 18:00 |


---

After this the system will show you the summary of the template, and you can go back and edit or go ahead and create the Component.

After clicking on "Create" the registering of the Component will start. If no errors occurred it will go through the 3 phases (Fetching, Publishing and Registering) and will give you the links to the newly created Repository and the component in the Catalog.

The new repository will contain a standard Spark project.

**Be careful not to delete the `catalog-info.yml` as well as keep the project structure as given.**